<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kendall Allen">
<meta name="dcterms.date" content="2024-04-07">
<meta name="description" content="Seeing how AI Interprets my Facial Expressions">

<title>Prompting Large Language Models - About (my) Face</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Prompting Large Language Models</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kendallen"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">About (my) Face</h1>
                  <div>
        <div class="description">
          Seeing how AI Interprets my Facial Expressions
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">LLMs</div>
                <div class="quarto-category">Nonverbal Cues</div>
                <div class="quarto-category">Interpretaion</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Kendall Allen </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 7, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p><img src="emotions.png" width="300px"></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction:</h2>
<p>After experimenting with ChatGPT’s reaction to nonverbal cues in conversation, MattGPT himself was kind enough to direct me towards a software that would iterate on this process; an LLaVA program that interprets images either uploaded by a user or taken on a personal webcam. As described by the homepage, “LLaVA represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA.” Here is a link to the program I used: https://llava-vl.github.io/</p>
<p>I knew I had to try it out, so I started by prompting it with a couple of basic emotions.</p>
</section>
<section id="back-to-the-basics" class="level2">
<h2 class="anchored" data-anchor-id="back-to-the-basics">Back to the Basics:</h2>
<p>To kick off my exploration, I wanted to see how accurately LLaVA could interpret expressions that were seemingly more obvious, being faces of surprise and disgust.</p>
<section id="surprised" class="level3">
<h3 class="anchored" data-anchor-id="surprised">Surprised:</h3>
<section id="prompt" class="level4">
<h4 class="anchored" data-anchor-id="prompt">Prompt:</h4>
<p>How would you interpret this person’s emotions?</p>
<p><img src="surprised.jpg" width="300px"></p>
</section>
<section id="response" class="level4">
<h4 class="anchored" data-anchor-id="response">Response:</h4>
<p>The person in the image appears to be expressing surprise or shock. This is indicated by the wide-open eyes and the hand covering the mouth, which are common nonverbal cues for these emotions. The facial expression and body language suggest that the person is reacting to something unexpected or startling.</p>
</section>
</section>
<section id="disgusted" class="level3">
<h3 class="anchored" data-anchor-id="disgusted">Disgusted:</h3>
<section id="prompt-how-would-you-interpret-this-persons-emotions" class="level4">
<h4 class="anchored" data-anchor-id="prompt-how-would-you-interpret-this-persons-emotions">Prompt: How would you interpret this person’s emotions?</h4>
<p><img src="disgusted.jpg" width="300px"></p>
</section>
<section id="response-1" class="level4">
<h4 class="anchored" data-anchor-id="response-1">Response:</h4>
<p>The person in the image appears to have a concerned or puzzled expression. The furrowed brow and slight frown suggest that they might be thinking about something that is causing them to feel uncertain or perplexed. Their eyes are focused and seem to be looking directly at the camera, which could indicate that they are engaged in a conversation or trying to understand something.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Observations
</div>
</div>
<div class="callout-body-container callout-body">
<p>Wow! Even though these emotions seem pretty obvious, I never fail to be impressed when AI shows off its skills. I think that the ability to explain not only what is present in a picture but also a person’s facial expressions and their implications will be a really useful tool for people with vision issues, children, or neurodivergent people who find it challenging to interpret nonverbal/implied social cues. LLaVA did a strong job of not only describing the displayed emotion but also “citing” its reasoning, eg; “the furrowed brow and slight frown suggest […] .” While it didn’t quite get that I was intending to portray disgust in the second prompt, I think its analysis was really fair given that I provided no context for situational factors that could be contributing to my face.</p>
</div>
</div>
</section>
</section>
</section>
<section id="emotions-as-linked-to-behaviors" class="level2">
<h2 class="anchored" data-anchor-id="emotions-as-linked-to-behaviors">Emotions as linked to Behaviors:</h2>
<p>After getting warmed up with basic expressions, I wanted to see how the program interpreted my emotions when I was doing certain behaviors, with one serving as a more universal gesture (the praying hands) and the other being exclusive to one region (the middle finger specifically within the US).</p>
<section id="praying" class="level3">
<h3 class="anchored" data-anchor-id="praying">Praying:</h3>
<section id="prompt-1" class="level4">
<h4 class="anchored" data-anchor-id="prompt-1">Prompt:</h4>
<p>How would you interpret this person’s emotions?</p>
<p><img src="praying.jpg" width="300px"></p>
</section>
<section id="response-2" class="level4">
<h4 class="anchored" data-anchor-id="response-2">Response:</h4>
<p>The person in the image appears to be in a state of deep concentration or prayer. Her eyes are closed, and her hands are clasped together, which often signifies a moment of reflection, supplication, or meditation. Her facial expression is calm and focused, suggesting that she is engaged in a personal or spiritual practice.</p>
</section>
</section>
<section id="middle-finger" class="level3">
<h3 class="anchored" data-anchor-id="middle-finger">Middle Finger:</h3>
<section id="prompt-2" class="level4">
<h4 class="anchored" data-anchor-id="prompt-2">Prompt:</h4>
<p>How would you interpret this person’s emotions?</p>
<p><img src="middle_finger.jpg" width="300px"></p>
</section>
<section id="response-3" class="level4">
<h4 class="anchored" data-anchor-id="response-3">Response:</h4>
<p>The person in the image appears to be expressing a negative emotion, possibly frustration or annoyance. The furrowed brow, the slight pout, and the gesture of pointing the finger upwards with a closed fist can all be interpreted as signs of displeasure or dissatisfaction. The context of the image, such as the background or any additional information, could provide more insight into the specific circumstances that might be causing this emotion.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Observations
</div>
</div>
<div class="callout-body-container callout-body">
<p>Once again it did a great job interpreting the emotions, even when they were linked to a behavior! I think it’s funny how LLaVA described the middle finger gesture in a very logical way rather than calling it what it was. I also just realized that though it begins the responses with “this person,” it goes on to use she/her pronouns for me– there is defintiely an interesting opportunity here to see how it interprets different forms of gender expression in a later blog post. Also, I’m noticing that the program keeps its responses rather broad often noting that further information/context should be considered to fully understand an image. This seems to fit in line with a lot of AI responses in general, though, so I understand.</p>
</div>
</div>
</section>
</section>
</section>
<section id="becoming-evaluative" class="level2">
<h2 class="anchored" data-anchor-id="becoming-evaluative">Becoming Evaluative:</h2>
<p>For my final prompts, I wanted to see if I could push the platform to be a bit more subjective, entering the realm of subjective things like beauty and deception.</p>
<section id="attractiveness" class="level3">
<h3 class="anchored" data-anchor-id="attractiveness">Attractiveness:</h3>
<section id="prompt-3" class="level4">
<h4 class="anchored" data-anchor-id="prompt-3">Prompt:</h4>
<p>Do you think this person is attractive?</p>
<p><img src="pretty.jpg" width="300px"></p>
</section>
<section id="response-4" class="level4">
<h4 class="anchored" data-anchor-id="response-4">Response:</h4>
<p>Beauty is subjective, and what one person finds attractive, another may not. The girl in the image appears to be a young woman with a clear complexion, and she is wearing a simple necklace. She has a neutral expression on her face. If you’re asking for a personal opinion, I would say she has a pleasant appearance, but it’s important to remember that beauty is in the eye of the beholder.</p>
</section>
</section>
<section id="lying" class="level3">
<h3 class="anchored" data-anchor-id="lying">Lying:</h3>
<section id="prompt-4" class="level4">
<h4 class="anchored" data-anchor-id="prompt-4">Prompt:</h4>
<p>If this person told me they liked my shoes, should I believe them?</p>
<p><img src="shoes.jpg" width="300px"></p>
</section>
<section id="response-5" class="level4">
<h4 class="anchored" data-anchor-id="response-5">Response:</h4>
<p>The person in the image appears to have a somewhat displeased or skeptical expression on their face. If they made this face while saying they liked your shoes, it might be reasonable to question the sincerity of their statement. However, it’s important to consider that facial expressions can be complex and may not always accurately reflect a person’s true feelings or intentions. It’s also possible that the person is simply making a playful or sarcastic comment. Without additional context, it’s difficult to determine the sincerity of their statement.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Observations
</div>
</div>
<div class="callout-body-container callout-body">
<p>Okay, I’m flattered! This is genuinely the first time I’ve seen an AI platform say “if you’re asking for a personal opinon […]” and then actually give one! It was interested that it noted my “clear complexion,” as I wonder what exact factors go into it denoting a face as “pleasant.” I tried to do a little pose so it knew I was positioning myself for a picture rather than offering some kind of emotion, but it noted that my expression was neutral rather than modeled, which was interesting. I also really liked its response to my question about the person’s authenticity of their compliment, as it acknowledged the context of the image that contradicted the provided statement while also leaving room for further information/interpretation.</p>
</div>
</div>
</section>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion:</h2>
<p>I had SO much fun messing around with this program, and definitely think there is much more opportunity for exploration; gender expression, group photos, and more vague expressions come immediately to mind. I will note that I tried to ask it to identify my race, and it denied this request several times despite interpreting me as a woman, so I would be interested to see which identities it is okay to guess about and which it refuses to engage with. I wish I could submit videos doing more of the actions I prompted in Blog 9 such as playing with my hair or changing the intonation of my voice, but I’m sure something like that will be created in no time! For now, I had a really fascinating experience with this version.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/kendallen\.github\.io\/comm4190_S24_Using_LLMs_Blog\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>